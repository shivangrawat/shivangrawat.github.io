<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Dynamic Divisive Normalization - NeurIPS</title>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Merriweather:wght@700&display=swap"
        rel="stylesheet">

    <!-- Font Awesome for Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
        integrity="sha512-Fo3rlrZj/k7ujTTX5nH6xuthiyU9E9QeNwwiofXf9La7R5LsCpQ9Ong8/PbOrfLtb/yGT+BY3+Aqdv7WEVPVkQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />

    <!-- External CSS -->
    <link rel="stylesheet" href="styles.css" />
</head>

<body>
    <header>
        <h1 class="paper-title">Unconditional stability of a recurrent neural circuit implementing divisive normalization</h1>
        <p class="conference"><strong>NeurIPS 2024</strong></p>
        <p class="authors"><strong>Authors:</strong> Shivang Rawat, David J. Heeger, Stefano Martiniani</p>
        <p class="affiliations"><em>New York University</em></p>
    
        <div class="links">
            <button class="arxiv-button" onclick="window.open('https://arxiv.org/abs/2409.18946','_blank')">
                <img src="figures/arxiv.jpg" alt="ArXiv Logo" class="icon" /> ArXiv
            </button>
            <button class="github-button" onclick="window.open('https://github.com/martiniani-lab/dynamic-divisive-norm','_blank')">
                <img src="figures/GitHub_Logo.png" alt="GitHub Logo" class="icon" /> GitHub
            </button>
            <button class="poster-button" onclick="window.open('https://shivangrawat.github.io/dynamic-divisive-norm/figures/neurips_poster.png','_blank')">
                <i class="fas fa-file-alt"></i> Poster
            </button>
        </div>
    </header>
    
    

    <main>
        <section class="representative-image">
            <figure>
                <img src="figures/github_image.png" alt="Representative figure of the model" />
            </figure>
        </section>

        <section class="tl-dr">
            <h2>TL;DR</h2>
            <p>We introduce a principled approach of adding normalization in Recurrent Neural Networks (RNNs). Embedding
                normalization leads to unconditional (independent of parameter and input values) stability of the
                dynamical system, simplifying training and improving interpretability.
            </p>
        </section>

        <section class="abstract" id="abstract">
            <h2>Abstract</h2>
            <p>Stability in recurrent neural models poses a significant challenge, particularly in developing
                biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical
                circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical
                system, leading to an optimization problem with nonlinear stability constraints that are difficult to
                impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack
                biological plausibility and interpretability. In this work, we address these challenges by linking
                dynamic divisive normalization (DN) to the stability of ORGaNICs, a biologically plausible recurrent
                cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of
                neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property
                of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight
                matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators,
                which enables us to derive the circuit's energy function, providing a normative principle of what the
                circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we
                prove the stability of the 2D model and demonstrate empirically that stability holds in higher
                dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without
                gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which
                address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's
                performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on
                static image classification tasks and perform comparably to LSTMs on sequential tasks.</p>
        </section>

        <section class="introduction" id="introduction">
            <h2>Introduction</h2>
            <p>Recurrent neural networks are powerful tools for modeling sequential data, yet their lack of biological
                plausibility and complex training dynamics have limited their interpretability. Biological circuits, on
                the other hand, offer rich insights into stability and adaptability but are often difficult to train
                directly due to complex nonlinear interactions. Our work aims to unify these perspectives by
                incorporating
                dynamic divisive normalization into a recurrent circuit model, yielding unconditional stability that
                simplifies both theoretical analysis and practical training.</p>
        </section>

        <section class="results" id="results">
            <h2>Results</h2>
            <p>We empirically validate the unconditional stability property of the ORGaNICs framework by testing it on a
                range of tasks, including static image classification and sequential MNIST. The model not only avoids
                exploding and vanishing gradients, but also achieves competitive performance with LSTMs on sequential
                tasks. Visualizations of the circuitâ€™s energy function demonstrate how the model maintains stable
                attractors, offering interpretable dynamics that align with biological principles. Additional figures
                and
                benchmarks can be accessed via the links provided above.</p>
        </section>

        <section class="citation" id="citation">
            <h2>BibTeX Citation</h2>
            <pre>
@article{rawat2024unconditional,
    title={Unconditional stability of a recurrent neural circuit implementing divisive normalization},
    author={Rawat, Shivang and Heeger, David J and Martiniani, Stefano},
    journal={arXiv preprint arXiv:2409.18946},
    year={2024}
    }
            </pre>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Shivang Rawat</p>
    </footer>

    <script src="script.js"></script>
</body>

</html>