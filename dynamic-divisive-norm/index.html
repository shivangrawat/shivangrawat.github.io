<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Dynamic Divisive Normalization - NeurIPS</title>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Merriweather:wght@700&display=swap"
        rel="stylesheet">

    <!-- Font Awesome for Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
        integrity="sha512-Fo3rlrZj/k7ujTTX5nH6xuthiyU9E9QeNwwiofXf9La7R5LsCpQ9Ong8/PbOrfLtb/yGT+BY3+Aqdv7WEVPVkQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />

    <!-- External CSS -->
    <link rel="stylesheet" href="styles.css" />
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
    </script>
</head>


<body>
    <header>
        <h1 class="paper-title">Unconditional stability of a recurrent neural circuit implementing divisive
            normalization</h1>
        <p class="conference"><strong>NeurIPS 2024</strong></p>
        <p class="authors">
            <strong>Authors:</strong>
            <a href="https://www.linkedin.com/in/shivang-rawat-b54b6a151/" target="_blank" class="author-link">Shivang
                Rawat</a>,
            David J. Heeger and Stefano Martiniani
        </p>
        <p class="affiliations"><em>New York University</em></p>

        <div class="links">
            <button class="pdf-button" onclick="window.open('https://openreview.net/pdf?id=5lLb7aXRN9','_blank')">
                <img src="figures/pdf_logo.png" alt="GitHub" />
            </button>
            <button class="arxiv-button" onclick="window.open('https://arxiv.org/abs/2409.18946','_blank')">
                <img src="figures/arxiv.jpg" alt="ArXiv" />
            </button>
            <button class="github-button"
                onclick="window.open('https://github.com/martiniani-lab/dynamic-divisive-norm','_blank')">
                <img src="figures/GitHub_Logo.png" alt="GitHub" />
            </button>
            <button class="poster-button"
                onclick="window.open('https://shivangrawat.github.io/dynamic-divisive-norm/figures/neurips_poster.png','_blank')">
                <img src="figures/poster_logo.png" alt="Poster" />
            </button>
        </div>
    </header>


    <main>
        <section class="representative-image">
            <figure>
                <img src="figures/github_image.png" alt="Representative figure of the model" />
            </figure>
        </section>

        <section class="tl-dr">
            <h2>TL;DR</h2>
            <p>We introduce a principled approach of adding normalization in Recurrent Neural Networks (RNNs). Embedding
                normalization leads to unconditional (independent of model parameters and input values) stability of the
                dynamical system, simplifying training and improving interpretability.
            </p>
        </section>

        <section class="abstract" id="abstract">
            <h2>Abstract</h2>
            <p>Stability in recurrent neural models poses a significant challenge, particularly in developing
                biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical
                circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical
                system, leading to an optimization problem with nonlinear stability constraints that are difficult to
                impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack
                biological plausibility and interpretability. In this work, we address these challenges by linking
                dynamic divisive normalization (DN) to the stability of ORGaNICs, a biologically plausible recurrent
                cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of
                neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property
                of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight
                matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators,
                which enables us to derive the circuit's energy function, providing a normative principle of what the
                circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we
                prove the stability of the 2D model and demonstrate empirically that stability holds in higher
                dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without
                gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which
                address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's
                performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on
                static image classification tasks and perform comparably to LSTMs on sequential tasks.</p>
        </section>

        <section class="introduction" id="introduction">
            <h2>Introduction</h2>
            <p>Our work explores how a biologically-inspired recurrent neural network (RNN) architecture, known as an
                Oscillatory Recurrent Gated Neural Integrator Circuits (ORGaNICs), can achieve guaranteed stability while
                implementing divisive normalization—a core computation observed throughout the brain’s sensory
                processing circuits. Unlike conventional RNNs, where normalization techniques are often added as ad hoc
                “patches” lacking conceptual grounding, ORGaNICs include a built-in normalization mechanism that ensures
                stable and robust activity dynamics, regardless of the network’s size or parameter settings. This
                stability, proven mathematically, opens the door to training these models directly on challenging
                sequence tasks via standard backpropagation methods, without the need for tricky workarounds. By
                bridging the gap between biological plausibility and machine learning performance, ORGaNICs offer a
                fresh perspective on building stable, trainable, and conceptually grounded neural architectures.</p>
        </section>

        <section class="model-description" id="model-description">
            <h2>ORGaNICs</h2>
            <p>
                In its simplest form, the two-neuron-types ORGaNICs model 
                <cite>[Heeger 2019, Heeger 2020]</cite> with \(n\) neurons of each type can be written as,
            </p>
            <p>
                \[
                \begin{split}
                    \boldsymbol{\tau}_y \odot \dot{\mathbf{y}} &= -\mathbf{y} + \mathbf{b} \odot \mathbf{z} 
                    + \left(\mathbf{1} - \mathbf{a}^{+}\right) \odot \mathbf{W}_r 
                    \left(\sqrt{\mathbf{y}^+} - \sqrt{\mathbf{y}^-}\right) \\
                    \boldsymbol{\tau}_a \odot \dot{\mathbf{a}} &= -\mathbf{a} + \mathbf{b}_0^2 \odot \boldsymbol{\sigma}^2 
                    + \mathbf{W} \left(\left(\mathbf{y}^+ + \mathbf{y}^-\right) \odot \mathbf{a}^{+2}\right)
                \end{split}
                \]
            </p>
            <p>
                where \(\mathbf{y} \in \mathbb{R}^n\) and \(\mathbf{a} \in \mathbb{R}^n\) are the membrane potentials 
                (relative to an arbitrary threshold potential that we take to be \(0\)) of the excitatory (\(\mathbf{y}\)) 
                and inhibitory (\(\mathbf{a}\)) neurons, evolving according to the dynamical equations defined above 
                with \(\dot{\mathbf{y}}\) and \(\dot{\mathbf{a}}\) denoting the time derivatives. The notation 
                \(\odot\) denotes element-wise multiplication of vectors, and squaring, rectification, square-root, 
                and division are also performed element-wise. \(\mathbf{1}\) is an \(n\)-dimensional vector with all 
                entries equal to 1. \(\mathbf{z} \in \mathbb{R}^n\) is the input drive to the circuit and is a 
                weighted sum of the input, \(\mathbf{x} \in \mathbb{R}^m\), i.e., \(\mathbf{z} = \mathbf{W}_{zx} \mathbf{x}\).
            </p>
            <p>
                The firing rates, \(\mathbf{y}^\pm = \lfloor\pm\mathbf{y}\rfloor^2\) and 
                \(\mathbf{a}^+ = \sqrt{\lfloor\mathbf{a}\rfloor}\), are rectified (\(\lfloor .\rfloor\)) power 
                functions of the underlying membrane potentials. For the derivation of a general model with 
                arbitrary power-law exponents, including the Eq.~(1), see Appendix A. Note that the term 
                \(\sqrt{\mathbf{y}^+} - \sqrt{\mathbf{y}^-}\) serves the purpose of defining a mechanism for 
                reconstructing the membrane potential (which can be negative, depending on the sign of the input) 
                from the firing rates \(\mathbf{y}^\pm\) that are strictly nonnegative. \(\mathbf{y}^+\) and 
                \(\mathbf{y}^-\) are the firing rates of neurons with complementary receptive fields such that 
                they encode inputs with positive and negative signs, respectively. Note that only one of these 
                neurons fires at a given time. In ORGaNICs, these neurons have a single dynamical equation for 
                their membrane potentials, where the sign of \(\mathbf{y}\) indicates which neuron is active. 
                Neurons with such complementary (anti-phase) receptive fields are found adjacent to each other 
                in the visual cortex <cite>[Liu 1992]</cite>, and we hypothesize that such complementary neurons 
                are ubiquitous throughout the neocortex.
            </p>
            <p>
                \(\mathbf{b} \in {\mathbb{R}^+_*}^n\) and \(\mathbf{b}_0 \in {\mathbb{R}^+_*}^n\) are the input gains 
                for the external inputs \(\mathbf{z}\) and \(\boldsymbol{\sigma}\) fed to neurons \(\mathbf{y}\) 
                and \(\mathbf{a}\), respectively. \({\mathbb{R}^+_*}\) is the set of positive real numbers, 
                \(\{x \in \mathbb{R} \, | \,  x > 0\}\). \(\boldsymbol{\sigma} \in {\mathbb{R}^+_*}^n\) determines 
                the semisaturation of the responses of neurons \(\mathbf{y}\) by contributing to the depolarization 
                of neurons \(\mathbf{a}\). \(\boldsymbol{\tau}_y \in {\mathbb{R}^+_*}^n\) and 
                \(\boldsymbol{\tau}_a \in {\mathbb{R}^+_*}^n\) represent the time constants of \(\mathbf{y}\) and 
                \(\mathbf{a}\) neurons.
            </p>
        </section>

        <section class="results" id="results">
            <h2>Results</h2>
            <p>We empirically validate the unconditional stability property of the ORGaNICs framework by testing it on a
                range of tasks, including static image classification and sequential MNIST. The model not only avoids
                exploding and vanishing gradients, but also achieves competitive performance with LSTMs on sequential
                tasks. Visualizations of the circuit’s energy function demonstrate how the model maintains stable
                attractors, offering interpretable dynamics that align with biological principles. Additional figures
                and
                benchmarks can be accessed via the links provided above.</p>
        </section>

        <section class="citation" id="citation">
            <h2>BibTeX Citation</h2>
            <pre>
@article{rawat2024unconditional,
    title={Unconditional stability of a recurrent neural circuit implementing divisive normalization},
    author={Rawat, Shivang and Heeger, David J and Martiniani, Stefano},
    journal={arXiv preprint arXiv:2409.18946},
    year={2024}
    }
            </pre>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Shivang Rawat</p>
    </footer>

    <script src="script.js"></script>
</body>

</html>